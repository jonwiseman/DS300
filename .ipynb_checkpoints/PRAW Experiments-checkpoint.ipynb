{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Create Reddit Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'art'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='eQt4_moPBnQ8xg',\n",
    "                     client_secret='t8WuNRzCPPBUkBJbyV10MO3nWr0',\n",
    "                     password='AnimeTiddies5',\n",
    "                     user_agent='Windows:com.example.myredditapp:v1.0.0 (by /u/dig_bick69_420)',\n",
    "                     username='dig_bick69_420')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(fr'C:\\Users\\jonat\\Desktop\\Data Mining\\Project\\Data\\Text\\{cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = []\n",
    "\n",
    "with open(f'{cat}_list.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        subreddits.append(line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape all comments from top 10 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = reddit.subreddit(subreddits[0])\n",
    "submissions = sub.top(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "\n",
    "for submission in submissions:\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    count = 0\n",
    "    for top_level_comment in submission.comments:\n",
    "        if count < 30:\n",
    "            comments.append(top_level_comment.body)\n",
    "            for second_level_comment in top_level_comment.replies: \n",
    "                comments.append(second_level_comment.body)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_list = []\n",
    "\n",
    "for comment in comments:\n",
    "    cleaned_comment = clean_text(comment)\n",
    "    doc = nlp(cleaned_comment)\n",
    "    words = []\n",
    "    for word in doc:\n",
    "        if not word.is_stop and not word.is_punct:\n",
    "            words.append(lemmatizer(word.text, word.pos_)[0])\n",
    "            \n",
    "    new_word_list.extend(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle and dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_word_list, open(fr'C:\\Users\\jonat\\Desktop\\Data Mining\\Project\\Data\\Text\\{cat}\\{sub.display_name}.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_name in subreddits:\n",
    "    sub = reddit.subreddit(sub_name)\n",
    "    submissions = sub.top(limit=10)\n",
    "    comments = []\n",
    "\n",
    "    for submission in submissions:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        count = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            if count < 30:\n",
    "                comments.append(top_level_comment.body)\n",
    "                for second_level_comment in top_level_comment.replies: \n",
    "                    comments.append(second_level_comment.body)\n",
    "                count += 1\n",
    "                \n",
    "    new_word_list = []\n",
    "\n",
    "    for comment in comments:\n",
    "        cleaned_comment = clean_text(comment)\n",
    "        doc = nlp(cleaned_comment)\n",
    "        words = []\n",
    "        for word in doc:\n",
    "            if not word.is_stop and not word.is_punct:\n",
    "                words.append(lemmatizer(word.text, word.pos_)[0])\n",
    "\n",
    "        new_word_list.extend(words)\n",
    "        \n",
    "    pickle.dump(new_word_list, open(fr'C:\\Users\\jonat\\Desktop\\Data Mining\\Project\\Data\\Text\\{cat}\\{sub.display_name}.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return text\n",
    "    cleaned = re.sub(r'\\s\\s+', ' ', text)\n",
    "    cleaned = re.sub(r'\\n', '', cleaned)\n",
    "    cleaned = re.sub(r'\\s([,.?!;)])', r'\\1', cleaned)\n",
    "    cleaned = re.sub(r'No. (\\d)', r'number \\1', cleaned)\n",
    "    \n",
    "    return cleaned.lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
